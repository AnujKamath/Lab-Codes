{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lplab/anaconda3/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('chapter_4').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      " |-- _c4: integer (nullable = true)\n",
      " |-- _c5: integer (nullable = true)\n",
      " |-- _c6: integer (nullable = true)\n",
      " |-- _c7: integer (nullable = true)\n",
      " |-- _c8: integer (nullable = true)\n",
      " |-- _c9: integer (nullable = true)\n",
      " |-- _c10: integer (nullable = true)\n",
      " |-- _c11: integer (nullable = true)\n",
      " |-- _c12: integer (nullable = true)\n",
      " |-- _c13: integer (nullable = true)\n",
      " |-- _c14: integer (nullable = true)\n",
      " |-- _c15: integer (nullable = true)\n",
      " |-- _c16: integer (nullable = true)\n",
      " |-- _c17: integer (nullable = true)\n",
      " |-- _c18: integer (nullable = true)\n",
      " |-- _c19: integer (nullable = true)\n",
      " |-- _c20: integer (nullable = true)\n",
      " |-- _c21: integer (nullable = true)\n",
      " |-- _c22: integer (nullable = true)\n",
      " |-- _c23: integer (nullable = true)\n",
      " |-- _c24: integer (nullable = true)\n",
      " |-- _c25: integer (nullable = true)\n",
      " |-- _c26: integer (nullable = true)\n",
      " |-- _c27: integer (nullable = true)\n",
      " |-- _c28: integer (nullable = true)\n",
      " |-- _c29: integer (nullable = true)\n",
      " |-- _c30: integer (nullable = true)\n",
      " |-- _c31: integer (nullable = true)\n",
      " |-- _c32: integer (nullable = true)\n",
      " |-- _c33: integer (nullable = true)\n",
      " |-- _c34: integer (nullable = true)\n",
      " |-- _c35: integer (nullable = true)\n",
      " |-- _c36: integer (nullable = true)\n",
      " |-- _c37: integer (nullable = true)\n",
      " |-- _c38: integer (nullable = true)\n",
      " |-- _c39: integer (nullable = true)\n",
      " |-- _c40: integer (nullable = true)\n",
      " |-- _c41: integer (nullable = true)\n",
      " |-- _c42: integer (nullable = true)\n",
      " |-- _c43: integer (nullable = true)\n",
      " |-- _c44: integer (nullable = true)\n",
      " |-- _c45: integer (nullable = true)\n",
      " |-- _c46: integer (nullable = true)\n",
      " |-- _c47: integer (nullable = true)\n",
      " |-- _c48: integer (nullable = true)\n",
      " |-- _c49: integer (nullable = true)\n",
      " |-- _c50: integer (nullable = true)\n",
      " |-- _c51: integer (nullable = true)\n",
      " |-- _c52: integer (nullable = true)\n",
      " |-- _c53: integer (nullable = true)\n",
      " |-- _c54: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_without_header = spark.read.option(\"inferSchema\", True).option(\"header\", False).csv(\"covtype.data\")\n",
    "data_without_header.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Elevation=2596, Aspect=51, Slope=3, Horizontal_Distance_To_Hydrology=258, Vertical_Distance_To_Hydrology=0, Horizontal_Distance_To_Roadways=510, Hillshade_9am=221, Hillshade_Noon=232, Hillshade_3pm=148, Horizontal_Distance_To_Fire_Points=6279, Wilderness_Area_0=1, Wilderness_Area_1=0, Wilderness_Area_2=0, Wilderness_Area_3=0, Soil_Type_0=0, Soil_Type_1=0, Soil_Type_2=0, Soil_Type_3=0, Soil_Type_4=0, Soil_Type_5=0, Soil_Type_6=0, Soil_Type_7=0, Soil_Type_8=0, Soil_Type_9=0, Soil_Type_10=0, Soil_Type_11=0, Soil_Type_12=0, Soil_Type_13=0, Soil_Type_14=0, Soil_Type_15=0, Soil_Type_16=0, Soil_Type_17=0, Soil_Type_18=0, Soil_Type_19=0, Soil_Type_20=0, Soil_Type_21=0, Soil_Type_22=0, Soil_Type_23=0, Soil_Type_24=0, Soil_Type_25=0, Soil_Type_26=0, Soil_Type_27=0, Soil_Type_28=1, Soil_Type_29=0, Soil_Type_30=0, Soil_Type_31=0, Soil_Type_32=0, Soil_Type_33=0, Soil_Type_34=0, Soil_Type_35=0, Soil_Type_36=0, Soil_Type_37=0, Soil_Type_38=0, Soil_Type_39=0, Cover_Type=5.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "colnames = [\"Elevation\", \"Aspect\", \"Slope\", \\\n",
    "\"Horizontal_Distance_To_Hydrology\", \\\n",
    "\"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\",\n",
    "\\\n",
    "\"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\", \\\n",
    "\"Horizontal_Distance_To_Fire_Points\"] + \\\n",
    "[f\"Wilderness_Area_{i}\" for i in range(4)] + \\\n",
    "[f\"Soil_Type_{i}\" for i in range(40)] + \\\n",
    "[\"Cover_Type\"]\n",
    "data = data_without_header.toDF(*colnames).\\\n",
    "withColumn(\"Cover_Type\",\n",
    "col(\"Cover_Type\").cast(DoubleType()))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Elevation: int, Aspect: int, Slope: int, Horizontal_Distance_To_Hydrology: int, Vertical_Distance_To_Hydrology: int, Horizontal_Distance_To_Roadways: int, Hillshade_9am: int, Hillshade_Noon: int, Hillshade_3pm: int, Horizontal_Distance_To_Fire_Points: int, Wilderness_Area_0: int, Wilderness_Area_1: int, Wilderness_Area_2: int, Wilderness_Area_3: int, Soil_Type_0: int, Soil_Type_1: int, Soil_Type_2: int, Soil_Type_3: int, Soil_Type_4: int, Soil_Type_5: int, Soil_Type_6: int, Soil_Type_7: int, Soil_Type_8: int, Soil_Type_9: int, Soil_Type_10: int, Soil_Type_11: int, Soil_Type_12: int, Soil_Type_13: int, Soil_Type_14: int, Soil_Type_15: int, Soil_Type_16: int, Soil_Type_17: int, Soil_Type_18: int, Soil_Type_19: int, Soil_Type_20: int, Soil_Type_21: int, Soil_Type_22: int, Soil_Type_23: int, Soil_Type_24: int, Soil_Type_25: int, Soil_Type_26: int, Soil_Type_27: int, Soil_Type_28: int, Soil_Type_29: int, Soil_Type_30: int, Soil_Type_31: int, Soil_Type_32: int, Soil_Type_33: int, Soil_Type_34: int, Soil_Type_35: int, Soil_Type_36: int, Soil_Type_37: int, Soil_Type_38: int, Soil_Type_39: int, Cover_Type: double]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_data, test_data) = data.randomSplit([0.9, 0.1])\n",
    "train_data.cache()\n",
    "test_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------+\n",
      "|featureVector                                                                                        |\n",
      "+-----------------------------------------------------------------------------------------------------+\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1863.0,37.0,17.0,120.0,18.0,90.0,217.0,202.0,115.0,769.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,5,6,7,8,9,13,18],[1874.0,18.0,14.0,90.0,208.0,209.0,135.0,793.0,1.0,1.0])                 |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1879.0,28.0,19.0,30.0,12.0,95.0,209.0,196.0,117.0,778.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1888.0,33.0,22.0,150.0,46.0,108.0,209.0,185.0,103.0,735.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1889.0,353.0,30.0,95.0,39.0,67.0,153.0,172.0,146.0,600.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1896.0,337.0,12.0,30.0,6.0,175.0,195.0,224.0,168.0,732.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1898.0,34.0,23.0,175.0,56.0,134.0,210.0,184.0,99.0,765.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1899.0,355.0,22.0,153.0,43.0,124.0,178.0,195.0,151.0,819.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1901.0,311.0,9.0,30.0,2.0,190.0,195.0,234.0,179.0,726.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1903.0,5.0,13.0,42.0,4.0,201.0,203.0,214.0,148.0,708.0,1.0,1.0])    |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,16],[1903.0,67.0,16.0,108.0,36.0,120.0,234.0,207.0,100.0,969.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1905.0,19.0,27.0,134.0,58.0,120.0,188.0,171.0,108.0,636.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1905.0,33.0,27.0,90.0,46.0,150.0,204.0,171.0,89.0,725.0,1.0,1.0])   |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,16],[1905.0,77.0,21.0,90.0,38.0,120.0,241.0,196.0,75.0,1025.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1906.0,356.0,20.0,150.0,55.0,120.0,184.0,201.0,151.0,726.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1908.0,323.0,32.0,150.0,52.0,120.0,125.0,190.0,196.0,765.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1916.0,24.0,25.0,212.0,74.0,175.0,197.0,177.0,105.0,789.0,1.0,1.0]) |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1916.0,320.0,24.0,190.0,60.0,162.0,151.0,210.0,195.0,832.0,1.0,1.0])|\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,23],[1918.0,321.0,28.0,42.0,17.0,85.0,139.0,201.0,196.0,402.0,1.0,1.0])  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1919.0,30.0,22.0,67.0,9.0,256.0,208.0,188.0,107.0,661.0,1.0,1.0])   |\n",
      "+-----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "input_cols = colnames[:-1]\n",
    "vector_assembler = VectorAssembler(inputCols=input_cols,\n",
    "outputCol=\"featureVector\")\n",
    "assembled_train_data = vector_assembler.transform(train_data)\n",
    "assembled_train_data.select(\"featureVector\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_ffd2f0555da7, depth=5, numNodes=43, numClasses=8, numFeatures=54\n",
      "  If (feature 0 <= 3048.5)\n",
      "   If (feature 0 <= 2501.5)\n",
      "    If (feature 3 <= 15.0)\n",
      "     If (feature 12 <= 0.5)\n",
      "      If (feature 0 <= 2348.5)\n",
      "       Predict: 4.0\n",
      "      Else (feature 0 > 2348.5)\n",
      "       Predict: 2.0\n",
      "     Else (feature 12 > 0.5)\n",
      "      Predict: 6.0\n",
      "    Else (feature 3 > 15.0)\n",
      "     If (feature 16 <= 0.5)\n",
      "      Predict: 3.0\n",
      "     Else (feature 16 > 0.5)\n",
      "      If (feature 9 <= 1307.5)\n",
      "       Predict: 3.0\n",
      "      Else (feature 9 > 1307.5)\n",
      "       Predict: 4.0\n",
      "   Else (feature 0 > 2501.5)\n",
      "    If (feature 17 <= 0.5)\n",
      "     If (feature 0 <= 2952.5)\n",
      "      If (feature 15 <= 0.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 15 > 0.5)\n",
      "       Predict: 3.0\n",
      "     Else (feature 0 > 2952.5)\n",
      "      Predict: 2.0\n",
      "    Else (feature 17 > 0.5)\n",
      "     If (feature 0 <= 2712.5)\n",
      "      Predict: 3.0\n",
      "     Else (feature 0 > 2712.5)\n",
      "      If (feature 5 <= 1251.0)\n",
      "       Predict: 5.0\n",
      "      Else (feature 5 > 1251.0)\n",
      "       Predict: 2.0\n",
      "  Else (feature 0 > 3048.5)\n",
      "   If (feature 0 <= 3311.5)\n",
      "    If (feature 7 <= 240.5)\n",
      "     Predict: 1.0\n",
      "    Else (feature 7 > 240.5)\n",
      "     If (feature 3 <= 333.0)\n",
      "      If (feature 0 <= 3183.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 0 > 3183.5)\n",
      "       Predict: 1.0\n",
      "     Else (feature 3 > 333.0)\n",
      "      Predict: 2.0\n",
      "   Else (feature 0 > 3311.5)\n",
      "    If (feature 12 <= 0.5)\n",
      "     If (feature 3 <= 290.0)\n",
      "      If (feature 6 <= 207.5)\n",
      "       Predict: 1.0\n",
      "      Else (feature 6 > 207.5)\n",
      "       Predict: 7.0\n",
      "     Else (feature 3 > 290.0)\n",
      "      Predict: 1.0\n",
      "    Else (feature 12 > 0.5)\n",
      "     If (feature 45 <= 0.5)\n",
      "      Predict: 7.0\n",
      "     Else (feature 45 > 0.5)\n",
      "      If (feature 5 <= 919.0)\n",
      "       Predict: 7.0\n",
      "      Else (feature 5 > 919.0)\n",
      "       Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(seed = 1234, labelCol=\"Cover_Type\",\n",
    "featuresCol=\"featureVector\",\n",
    "predictionCol=\"prediction\")\n",
    "model = classifier.fit(assembled_train_data)\n",
    "print(model.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Elevation</th>\n",
       "      <td>0.839072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_3</th>\n",
       "      <td>0.035492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_1</th>\n",
       "      <td>0.030548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <td>0.026184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <td>0.022362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_31</th>\n",
       "      <td>0.017996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area_2</th>\n",
       "      <td>0.015323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <td>0.004583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_2</th>\n",
       "      <td>0.003516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <td>0.002662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <td>0.002261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_24</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_26</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_25</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_22</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_23</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_28</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_21</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_20</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_27</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_38</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_29</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_30</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_37</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_18</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_32</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_33</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_34</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_35</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_36</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_19</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_13</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_17</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_16</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Slope</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area_0</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area_1</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wilderness_Area_3</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_0</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_4</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_5</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_6</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_7</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_8</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_9</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_10</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_11</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_12</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aspect</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_14</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_15</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soil_Type_39</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    importance\n",
       "Elevation                             0.839072\n",
       "Soil_Type_3                           0.035492\n",
       "Soil_Type_1                           0.030548\n",
       "Hillshade_Noon                        0.026184\n",
       "Horizontal_Distance_To_Hydrology      0.022362\n",
       "Soil_Type_31                          0.017996\n",
       "Wilderness_Area_2                     0.015323\n",
       "Horizontal_Distance_To_Roadways       0.004583\n",
       "Soil_Type_2                           0.003516\n",
       "Hillshade_9am                         0.002662\n",
       "Horizontal_Distance_To_Fire_Points    0.002261\n",
       "Soil_Type_24                          0.000000\n",
       "Soil_Type_26                          0.000000\n",
       "Soil_Type_25                          0.000000\n",
       "Soil_Type_22                          0.000000\n",
       "Soil_Type_23                          0.000000\n",
       "Soil_Type_28                          0.000000\n",
       "Soil_Type_21                          0.000000\n",
       "Soil_Type_20                          0.000000\n",
       "Soil_Type_27                          0.000000\n",
       "Soil_Type_38                          0.000000\n",
       "Soil_Type_29                          0.000000\n",
       "Soil_Type_30                          0.000000\n",
       "Soil_Type_37                          0.000000\n",
       "Soil_Type_18                          0.000000\n",
       "Soil_Type_32                          0.000000\n",
       "Soil_Type_33                          0.000000\n",
       "Soil_Type_34                          0.000000\n",
       "Soil_Type_35                          0.000000\n",
       "Soil_Type_36                          0.000000\n",
       "Soil_Type_19                          0.000000\n",
       "Soil_Type_13                          0.000000\n",
       "Soil_Type_17                          0.000000\n",
       "Soil_Type_16                          0.000000\n",
       "Slope                                 0.000000\n",
       "Vertical_Distance_To_Hydrology        0.000000\n",
       "Hillshade_3pm                         0.000000\n",
       "Wilderness_Area_0                     0.000000\n",
       "Wilderness_Area_1                     0.000000\n",
       "Wilderness_Area_3                     0.000000\n",
       "Soil_Type_0                           0.000000\n",
       "Soil_Type_4                           0.000000\n",
       "Soil_Type_5                           0.000000\n",
       "Soil_Type_6                           0.000000\n",
       "Soil_Type_7                           0.000000\n",
       "Soil_Type_8                           0.000000\n",
       "Soil_Type_9                           0.000000\n",
       "Soil_Type_10                          0.000000\n",
       "Soil_Type_11                          0.000000\n",
       "Soil_Type_12                          0.000000\n",
       "Aspect                                0.000000\n",
       "Soil_Type_14                          0.000000\n",
       "Soil_Type_15                          0.000000\n",
       "Soil_Type_39                          0.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(model.featureImportances.toArray(),\n",
    "index=input_cols, columns=['importance']).\\\n",
    "sort_values(by=\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Cover_Type|prediction|probability                                                                                                                         |\n",
      "+----------+----------+------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|6.0       |3.0       |[0.0,3.128519584532599E-5,0.06901514203478913,0.6083406332123639,0.020241521711925916,0.0017519709673382556,0.30061944687773745,0.0]|\n",
      "|6.0       |4.0       |[0.0,0.0,0.005867014341590613,0.3011734028683181,0.5964797913950456,0.0,0.09647979139504563,0.0]                                    |\n",
      "|6.0       |3.0       |[0.0,3.128519584532599E-5,0.06901514203478913,0.6083406332123639,0.020241521711925916,0.0017519709673382556,0.30061944687773745,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.128519584532599E-5,0.06901514203478913,0.6083406332123639,0.020241521711925916,0.0017519709673382556,0.30061944687773745,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.128519584532599E-5,0.06901514203478913,0.6083406332123639,0.020241521711925916,0.0017519709673382556,0.30061944687773745,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.128519584532599E-5,0.06901514203478913,0.6083406332123639,0.020241521711925916,0.0017519709673382556,0.30061944687773745,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.128519584532599E-5,0.06901514203478913,0.6083406332123639,0.020241521711925916,0.0017519709673382556,0.30061944687773745,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.128519584532599E-5,0.06901514203478913,0.6083406332123639,0.020241521711925916,0.0017519709673382556,0.30061944687773745,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.128519584532599E-5,0.06901514203478913,0.6083406332123639,0.020241521711925916,0.0017519709673382556,0.30061944687773745,0.0]|\n",
      "|6.0       |3.0       |[0.0,3.128519584532599E-5,0.06901514203478913,0.6083406332123639,0.020241521711925916,0.0017519709673382556,0.30061944687773745,0.0]|\n",
      "+----------+----------+------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(assembled_train_data)\n",
    "predictions.select(\"Cover_Type\", \"prediction\", \"probability\").\\\n",
    "show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6850828367432688"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Cover_Type\",\n",
    "predictionCol=\"prediction\")\n",
    "evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
    "evaluator.setMetricName(\"f1\").evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+-----+----+---+---+-----+\n",
      "|Cover_Type|     1|     2|    3|   4|  5|  6|    7|\n",
      "+----------+------+------+-----+----+---+---+-----+\n",
      "|       1.0|119446| 65801|  102|   0| 20|  7| 5333|\n",
      "|       2.0| 42482|207574| 3663|   9|360| 57|  786|\n",
      "|       3.0|     0|  3920|27696| 473| 22|125|    0|\n",
      "|       4.0|     0|    96| 1205|1173|  0|  0|    0|\n",
      "|       5.0|     0|  7776|  347|   0|444|  0|    0|\n",
      "|       6.0|     0|  4511|10438| 148|  6|503|    0|\n",
      "|       7.0|  7789|   316|    0|   0|  0|  0|10362|\n",
      "+----------+------+------+-----+----+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = predictions.groupBy(\"Cover_Type\").\\\n",
    "pivot(\"prediction\", range(1,8)).count().\\\n",
    "na.fill(0.0).\\\n",
    "orderBy(\"Cover_Type\")\n",
    "confusion_matrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count_proportion=0.36465133176542575),\n",
       " Row(count_proportion=0.4874490908047955),\n",
       " Row(count_proportion=0.06163788982580929),\n",
       " Row(count_proportion=0.004730491978814127),\n",
       " Row(count_proportion=0.016380810340541885),\n",
       " Row(count_proportion=0.029839958699019103),\n",
       " Row(count_proportion=0.03531042658559437)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "def class_probabilities(data):\n",
    "    total = data.count()\n",
    "    return data.groupBy(\"Cover_Type\").count().\\\n",
    "    orderBy(\"Cover_Type\").\\\n",
    "    select(col(\"count\").cast(DoubleType())).\\\n",
    "    withColumn(\"count_proportion\", col(\"count\")/total).\\\n",
    "    select(\"count_proportion\").collect()\n",
    "train_prior_probabilities = class_probabilities(train_data)\n",
    "test_prior_probabilities = class_probabilities(test_data)\n",
    "train_prior_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3773114942184577"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_prior_probabilities = [p[0] for p in train_prior_probabilities]\n",
    "test_prior_probabilities = [p[0] for p in test_prior_probabilities]\n",
    "sum([train_p * cv_p for train_p, cv_p in zip(train_prior_probabilities,\n",
    "test_prior_probabilities)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"featureVector\")\n",
    "classifier = DecisionTreeClassifier(seed=1234, labelCol=\"Cover_Type\",\n",
    "featuresCol=\"featureVector\",\n",
    "predictionCol=\"prediction\")\n",
    "pipeline = Pipeline(stages=[assembler, classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "paramGrid = ParamGridBuilder(). \\\n",
    "addGrid(classifier.impurity, [\"gini\", \"entropy\"]). \\\n",
    "addGrid(classifier.maxDepth, [1, 20]). \\\n",
    "addGrid(classifier.maxBins, [40, 300]). \\\n",
    "addGrid(classifier.minInfoGain, [0.0, 0.05]). \\\n",
    "build()\n",
    "multiclassEval = MulticlassClassificationEvaluator(). \\\n",
    "setLabelCol(\"Cover_Type\"). \\\n",
    "setPredictionCol(\"prediction\"). \\\n",
    "setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "validator = TrainValidationSplit(seed=1234,\n",
    "estimator=pipeline,\n",
    "evaluator=multiclassEval,\n",
    "estimatorParamMaps=paramGrid,\n",
    "trainRatio=0.9)\n",
    "validator_model = validator.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='DecisionTreeClassifier_f87d122ed8ec', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False,\n",
      " Param(parent='DecisionTreeClassifier_f87d122ed8ec', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 20,\n",
      " Param(parent='DecisionTreeClassifier_f87d122ed8ec', name='featuresCol', doc='features column name.'): 'featureVector',\n",
      " Param(parent='DecisionTreeClassifier_f87d122ed8ec', name='labelCol', doc='label column name.'): 'Cover_Type',\n",
      " Param(parent='DecisionTreeClassifier_f87d122ed8ec', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1,\n",
      " Param(parent='DecisionTreeClassifier_f87d122ed8ec', name='seed', doc='random seed.'): 1234,\n",
      " Param(parent='DecisionTreeClassifier_f87d122ed8ec', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10,\n",
      " Param(parent='DecisionTreeClassifier_f87d122ed8ec', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256,\n",
      " Param(parent='DecisionTreeClassifier_f87d122ed8ec', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0,\n",
      " Param(parent='DecisionTreeClassifier_f87d122ed8ec', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0,\n",
      " Param(parent='DecisionTreeClassifier_f87d122ed8ec', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 40,\n",
      " Param(parent='DecisionTreeClassifier_f87d122ed8ec', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'): '',\n",
      " Param(parent='DecisionTreeClassifier_f87d122ed8ec', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
      " Param(parent='DecisionTreeClassifier_f87d122ed8ec', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'entropy',\n",
      " Param(parent='DecisionTreeClassifier_f87d122ed8ec', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',\n",
      " Param(parent='DecisionTreeClassifier_f87d122ed8ec', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "best_model = validator_model.bestModel\n",
    "pprint(best_model.stages[1].extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator_model = validator.fit(train_data)\n",
    "metrics = validator_model.validationMetrics\n",
    "params = validator_model.getEstimatorParamMaps()\n",
    "metrics_and_params = list(zip(metrics, params))\n",
    "metrics_and_params.sort(key=lambda x: x[0], reverse=True)\n",
    "metrics_and_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.sort(reverse=True)\n",
    "print(metrics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclassEval.evaluate(best_model.transform(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "def unencode_one_hot(data):\n",
    "    wilderness_cols = ['Wilderness_Area_' + str(i) for i in range(4)]\n",
    "    wilderness_assembler = VectorAssembler().\\\n",
    "                            setInputCols(wilderness_cols).\\\n",
    "                            setOutputCol(\"wilderness\")\n",
    "    unhot_udf = udf(lambda v: v.toArray().tolist().index(1))\n",
    "    with_wilderness = wilderness_assembler.transform(data).\\\n",
    "      drop(*wilderness_cols).\\\n",
    "      withColumn(\"wilderness\", unhot_udf(col(\"wilderness\")).cast(IntegerType()))\n",
    "    soil_cols = ['Soil_Type_' + str(i) for i in range(40)]\n",
    "    soil_assembler = VectorAssembler().\\\n",
    "                    setInputCols(soil_cols).\\\n",
    "                    setOutputCol(\"soil\")\n",
    "    with_soil = soil_assembler.\\\n",
    "                transform(with_wilderness).\\\n",
    "                drop(*soil_cols).\\\n",
    "                withColumn(\"soil\", unhot_udf(col(\"soil\")).cast(IntegerType()))\n",
    "    return with_soil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unenc_train_data = unencode_one_hot(train_data)\n",
    "unenc_train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unenc_train_data.groupBy('wilderness').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "cols = unenc_train_data.columns\n",
    "input_cols = [c for c in cols if c!='Cover_Type']\n",
    "assembler = VectorAssembler().setInputCols(input_cols).setOutputCol(\"featureVector\")\n",
    "indexer = VectorIndexer().\\\n",
    "setMaxCategories(40).\\\n",
    "setInputCol(\"featureVector\").setOutputCol(\"indexedVector\")\n",
    "classifier = DecisionTreeClassifier().setLabelCol(\"Cover_Type\").\\\n",
    "                                        setFeaturesCol(\"indexedVector\").\\\n",
    "                                        setPredictionCol(\"prediction\")\n",
    "pipeline = Pipeline().setStages([assembler, indexer, classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(seed=1234, labelCol=\"Cover_Type\",\n",
    "featuresCol=\"indexedVector\",predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unenc_train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## LONGER TIME ##################################\n",
    "cols = unenc_train_data.columns\n",
    "input_cols = [c for c in cols if c!='Cover_Type']\n",
    "assembler = VectorAssembler().setInputCols(input_cols).\n",
    ",â†’setOutputCol(\"featureVector\")\n",
    "indexer = VectorIndexer().\\\n",
    "setMaxCategories(40).\\\n",
    "setInputCol(\"featureVector\").setOutputCol(\"indexedVector\")\n",
    "pipeline = Pipeline().setStages([assembler, indexer, classifier])\n",
    "paramGrid = ParamGridBuilder(). \\\n",
    "addGrid(classifier.impurity, [\"gini\", \"entropy\"]). \\\n",
    "addGrid(classifier.maxDepth, [1, 20]). \\\n",
    "addGrid(classifier.maxBins, [40, 300]). \\\n",
    "addGrid(classifier.minInfoGain, [0.0, 0.05]). \\\n",
    "build()\n",
    "multiclassEval = MulticlassClassificationEvaluator(). \\\n",
    "setLabelCol(\"Cover_Type\"). \\\n",
    "setPredictionCol(\"prediction\"). \\\n",
    "setMetricName(\"accuracy\")\n",
    "validator = TrainValidationSplit(seed=1234,\n",
    "estimator=pipeline,\n",
    "evaluator=multiclassEval,\n",
    "estimatorParamMaps=paramGrid,\n",
    "trainRatio=0.9)\n",
    "validator_model = validator.fit(unenc_train_data)\n",
    "best_model = validator_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = best_model.stages[2]\n",
    "feature_importance_list = list(zip(input_cols,\n",
    "forest_model.featureImportances.toArray()))\n",
    "feature_importance_list.sort(key=lambda x: x[1], reverse=True)\n",
    "pprint(feature_importance_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unenc_test_data = unencode_one_hot(test_data)\n",
    "best_model.transform(unenc_test_data.drop(\"Cover_Type\")).\\\n",
    "select(\"prediction\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
